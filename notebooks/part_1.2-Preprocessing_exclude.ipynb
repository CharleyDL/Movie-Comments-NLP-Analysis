{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>NLP : Movie Critics</center></h1>\n",
    "<h2><center>Part 1 : Preprocessing excluding 3 and 3.5 notes</center></h2>\n",
    "<center><img src=\"https://simplonline.co/_next/image?url=https%3A%2F%2Fsimplonline-v3-prod.s3.eu-west-3.amazonaws.com%2Fmedia%2Fimage%2Fjpg%2F1dea9fa5-b2da-4731-925d-b2280a28af3e.jpg&w=1280&q=75\" alt=\"iIllustration\" title=\"title\"  style=\"object-fit:cover; width:800px; height:250px;\"/></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __*Created by Charley lebarbier*__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives :  \n",
    "*En règle générale, le nombre d'avis sur un film peu être important et par conséquent le temps de lecture de chaque commentaire peut être une tâche lourde. Alors comment déterminer de manière rapide si un film a eu du succès auprès des spectateurs (ou pas) ? Dans ce contexte, l’idée du projet est d’utiliser des algorithmes d'apprentissage automatique pour la tâche d'analyse de sentiment des spectateurs via leur critique.*  \n",
    "\n",
    "*Tout d’abord, il sera question que récupérer les données directement du site d’Allociné. En d’autres termes, nous allons scraper les pages qui nous intéressent sur ce site à savoir les critiques des personnes pour le film Inception et Sonic 2.*  \n",
    "\n",
    "*En navigant sur la page des critiques, vous vous apercevrez que seules deux types d’information ici nous intéresse : la note du spectateur ainsi que son avis. Pourquoi la note ? Parce que nous allons entraîner un modèle de type supervisé et plus précisément un classifieur et donc la note va nous aider à récupérer la classe pour étiqueter le commentaire. Pour cela, nous considérerons qu’une note au-dessus de 3 est considérée comme satisfaisante. Sinon, l’avis est négatif. Ici, nous avons donc réduit le problème à une classification binaire.*  \n",
    "\n",
    "- Voici donc les étapes à réaliser :  \n",
    "Récupération des données • Préparation des données. • Préparation du modèle et des jeux de données (entrainement & test) • Analyse des résultats*  \n",
    "  \n",
    "  \n",
    "- __Etape 1 : Web Scraping des données d’avis de spectacteurs__\n",
    "    - De l’avis du spectateur, nous ne devons « scraper » que deux zones la note et le commentaire.  \n",
    "</br>\n",
    "- __Etape 2 : Préparation des données__\n",
    "    - Ayant maintenant nos jeux de données, il faut les préparer afin de pouvoir modéliser notre analyse de sentiments. Pour cela nous allons faire appel à plusieurs techniques :  \n",
    "        - Des expressions régulières pour retirer les bruits (ponctuation, etc.) des commentaires.  \n",
    "        - Du NLP pour tokeniser et réduire le corpus de chaque commentaire (afin par exemple de ne garder que les mots importants via les stopwords)*  \n",
    "            - Des sacs de mots afin de « transformer » nos mots en nombres qui pourront alors être exploités dans un algorithme de Machine learning*  \n",
    "            *Les commentaires sont maintenant filtrés à leur essentiel.*    \n",
    "</br>\n",
    "- __Etape 3 : Préparation des libellés__  \n",
    "    - Jusque là, à chaque commentaire est associé une note de 1 à 5 et non une classe binaire. Il nous faut donc convertir nos notes en : 1 pour avis positif et 0 : pour avis négatif  \n",
    "    - Note: N’oublions pas à la fin de retirer la note du jeu de données.  \n",
    "</br>\n",
    "- __Etape 4 : Finalisation de nos jeux de données__  \n",
    "    - Les données sont presque prêtes mais nos commentaires qui sont maintenant sous forme de sac de mots doivent être convertis en nombre. Pour cela, il va falloir vectoriser nos mots (technique des sacs de mots) :  \n",
    "        - Vous devriez avoir maintenant une belle matrice avec beaucoup de colonnes (qui correspond au nombre de mots du corpus)  \n",
    "        - Afficher deux _WordCloud_: *le WordCloud des avis positifs et celui des avis négatifs.*  \n",
    "</br>\n",
    "- __Etape 5 : Entraînement du modèle__  \n",
    "    - Nos données sont prêtes, nous allons pour ce premier exercice utiliser un algorithme de Regression Logistique comme ici il est question de classification binaire. Entraînons le modèle maintenant, et regardons sa précision par rapport au libellés connus.  \n",
    "</br>\n",
    "- __Etape 6 : Analyse des résultats__  \n",
    "    - Calculer l’accuracy et la matrice de confusion sur les données de test. Une fois que les résultats sont satisfaisants, vous pourrez maintenant tester sur des commentaires que vous et vos collègues ferons afin de vérifier le bon fonctionnement du programme.  \n",
    "</br>\n",
    "- __Etape 7 : Réalisation d'une application WEB avec Flask qui lit un commentaire de Film et détermine s'il est positif ou pas__  \n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "Pour Finir :\n",
    "Option 1 : voir si en enlevant des commentaires des mots qui ressortent le plus dans le nuages de mots des données d'apprentissage (quelque soit le target)  \n",
    "</br>\n",
    "Option 2 : Générer des commentaires plus cours. s'aider du nuage de mots (avis positifs, avis négatifs).  \n",
    "</br>\n",
    "Option 3 : tester sur plusieurs modèles de classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "----  \n",
    "----  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>\n",
    "<center><h2>Web Scraping</h2></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lancer le code dans 'modules' = allo_cine_scraping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>\n",
    "<center><h2>Working Environment Preparation</h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset includes 'viewer_reviews' on 5 films :\n",
    "- Sonic 2 (average note : 3,2)\n",
    "- Inception (average note : 4,5)\n",
    "- Brocéliande (average note : 1,0)\n",
    "- DragonBall Evolution (average note : 0,8)\n",
    "- Avatar (average note : 4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset\n",
    "df = pd.read_csv('../data/viewers_critic_movies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28700 entries, 0 to 28699\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Note     28700 non-null  object\n",
      " 1   Comment  28700 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 448.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>\n",
    "<center><h2>Set Preparation</h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First cleaning : delete the newline chars, the comments with only link\n",
    "# and replace cells containing only whitespaces by NaN\n",
    "df['Comment'] = df['Comment'].str.replace(r\"\\n\", \"\")\n",
    "df['Comment'] = df['Comment'].str.replace(r\"http\\S+\", \"\")\n",
    "df['Comment'] = df['Comment'].replace(['^\\s*$'], np.nan, regex = True)\n",
    "\n",
    "# Delete NaN Cells\n",
    "df.dropna(subset=['Comment'], inplace=True)\n",
    "\n",
    "# Reset the index\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Check result\n",
    "# with pd.option_context('display.max_rows', None):\n",
    "#     print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of critics for each note\n",
    "sns.countplot(data=df, x='Note')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(24,12)\n",
    "fig.suptitle(\"Number of reviews per rating\", fontweight =\"bold\", fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding the Target Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the target 'note' : >=4 - Positive ; < 3 - Negative\n",
    "def encodingTarget(target:str) -> None:\n",
    "    \"\"\"\n",
    "    Encode the target 'note' on this pattern : : >=3 - Positive ; <3 - Negative\n",
    "    @Params :\n",
    "        target      - required : object (column 'output' in dataframe)\n",
    "    \"\"\"\n",
    "\n",
    "    # Replace the comma to dot for conversion in float\n",
    "    df[target] = df[target].str.replace(\",\", \".\")\n",
    "\n",
    "    # Convert the note excluding 3 and 3,5 notes\n",
    "    for i in range(len(df[target])):\n",
    "        if float(df['Note'][i]) >= 4:\n",
    "            df['Note'][i] = 1\n",
    "        elif float(df['Note'][i]) < 3:\n",
    "            df['Note'][i] = 0\n",
    "\n",
    "encodingTarget('Note')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with 3 and 3.5 notes\n",
    "\n",
    "df.drop(df[ (df['Note']) == '3.0'].index, inplace = True)\n",
    "df.drop(df[ (df['Note']) == '3.5'].index, inplace = True)\n",
    "\n",
    "\n",
    "# Reset the index\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the number of critics after encoding\n",
    "ax = sns.countplot(data=df, y='Note', hue='Note', dodge=False)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(12,4)\n",
    "\n",
    "h,l = ax.get_legend_handles_labels()\n",
    "ax.legend(h, labels=[\"Avis Négatif < 3.0\", \"Avis Positif: >= 4\"], \n",
    "             title=\"Note\", loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new dataset with the same number of negative and positive reviews\n",
    "\n",
    "*For the division, we will stay on 12000 comments of each category to constitute a new dataset of 24000 entries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12000\n",
       "1    12000\n",
       "Name: Note, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def df_equalizer(target, n_sample) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Equals a dataframe with the same number of classes\n",
    "    @Params:\n",
    "        target          - required  : str (column where classes are)\n",
    "        n_sample        - required  : int (number of sample for equalization)\n",
    "    Example n_sample -> 6000 : for 2 classes, create a new_dataframe of \n",
    "    12000 sample (6000 / 6000)\n",
    "    \"\"\"\n",
    "\n",
    "    # Create new dataframe with number of sample requested    \n",
    "    new_df = df.groupby([target], group_keys=False).apply\\\n",
    "        (lambda grp: grp.sample(n=n_sample, random_state=42, replace=False))\n",
    "\n",
    "    # Shuffle the new dataframe (need : from sklearn.utils import shuffle)\n",
    "    new_df = shuffle(new_df, random_state=42)\n",
    "\n",
    "    # reindex the new_df\n",
    "    new_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return new_df\n",
    "\n",
    "new_df = df_equalizer('Note', 12_000)\n",
    "\n",
    "# Check the parity after equalization\n",
    "new_df['Note'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the result\n",
    "# with pd.option_context('display.max_rows', None):\n",
    "#     print(new_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Feature (*'Comment'*) and Target (*'Note'*) dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = new_df['Comment']    # feature\n",
    "y = new_df['Note']       # target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index to avoid errors during working loops\n",
    "\n",
    "x_train.reset_index(drop=True, inplace=True)\n",
    "x_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>\n",
    "<center><h2>PREPROCESSING</h2></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A partir de maintenant, nous travaillerons que sur le split 'x_train' afin d'entraîner le modèle. Le set y_train est préservé pour garder la mise en situation réelle, cad un texte non normalisé*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###---- Code to Check result ----###\n",
    "\n",
    "### Display all x_train ###\n",
    "# with pd.option_context('display.max_rows', None):\n",
    "    # print(x_train)\n",
    "\n",
    "### Display the entire row###\n",
    "# pd.options.display.max_rows\n",
    "# pd.set_option('display.max_colwidth', -1)\n",
    "# print(x_train[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower casing the x_train\n",
    "x_train = x_train.str.lower()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Removal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Warning**\n",
    "> voir si on garde les emojis :) ou :( qui peut donner une indication..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "# Reduce the different noise inside comments\n",
    "def reduce_text_noise(train_set:pd.Series) -> None:\n",
    "    \"\"\"\n",
    "    Remove all noise who can fine in text and delete empty cell create after\n",
    "    modification.\n",
    "\n",
    "    Treated elements :\n",
    "    - URL\n",
    "    - HTML Tag\n",
    "    - Non-ASCII\n",
    "    - Special character (emoji, symbol, graphic chars)\n",
    "    - Punctuation\n",
    "    - Numbers\n",
    "    - Extra Whitespaces\n",
    "\n",
    "    @Params:\n",
    "        train_set       - required : pd.Series        in general the x_train\n",
    "    \"\"\"\n",
    "\n",
    "    # Text Treatment\n",
    "    for i in range(0, len(train_set)):\n",
    "        # URL\n",
    "        train_set[i] = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", train_set[i])\n",
    "\n",
    "        # HTML Tag \n",
    "        html = re.compile(r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\")\n",
    "        train_set[i] = re.sub(html, \"\", train_set[i])\n",
    "\n",
    "        # Non-ASCII\n",
    "        #train_set[i] = re.sub(r'[^\\x00-\\x7f]', r'', train_set[i])\n",
    "\n",
    "        # Special Chars\n",
    "        # emoji_pattern = re.compile(\n",
    "        #     '['\n",
    "        #         u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "        #         u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "        #         u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "        #         u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "        #         u'\\U00002702-\\U000027B0'  # other symbols\n",
    "        #         u'\\U000024C2-\\U0001F251'  # other symbols\n",
    "        #     ']+',\n",
    "        # flags=re.UNICODE)\n",
    "        # train_set[i] = emoji_pattern.sub(r'', train_set[i])\n",
    "\n",
    "        # Numbers\n",
    "        # train_set[i] = ''.join([i for i in train_set[i] if not i.isdigit()])\n",
    "\n",
    "        # Punctuation\n",
    "        train_set[i] = re.sub(r'[]!\"\\'$€%&()*+,./:;=#@?[\\\\^_`{|}~-]+', \" \", train_set[i])\n",
    "\n",
    "        # Extra Whitespaces\n",
    "        train_set[i] = re.sub(' +', ' ', train_set[i])\n",
    "\n",
    "reduce_text_noise(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def informal_clean_text(train_set:pd.Series) -> None:\n",
    "    \"\"\"\n",
    "    Transcribes the abbreviations, slang words or english word in full french \n",
    "    word.\n",
    "    Example : LOL -> in french : 'mort de rire'\n",
    "    @Params:\n",
    "        train_set       - required : pd.Series        in general the x_train\n",
    "    \"\"\"\n",
    "\n",
    "    # Dictionary can be update\n",
    "    dict_abbr = {\n",
    "                    \"€\": \"euros\",\n",
    "                    \"dsl\": \"désolé\",\n",
    "                    \"koi\": \"quoi\",\n",
    "                    \"lol\": \"mort de rire\",\n",
    "                    \"mdr\": \"mort de rire\",\n",
    "                    \"nikel\": \"nickel\",\n",
    "                    \"pk\": \"pourquoi\",\n",
    "                    \"shit\": \"merde\",\n",
    "                }\n",
    "    sample_abbr = re.compile(r'(?<!\\w)(' + '|'\\\n",
    "        .join(re.escape(key) for key in dict_abbr.keys()) + r')(?!\\w)')\n",
    "\n",
    "    for i in range(0, len(train_set)):\n",
    "        train_set[i] = sample_abbr.sub(lambda x: dict_abbr[x.group()], train_set[i])\n",
    "\n",
    "informal_clean_text(x_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spell Checker\n",
    "\n",
    "*environ 11min 20sec d'exécution*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6880it [31:18,  3.66it/s]\n"
     ]
    }
   ],
   "source": [
    "def correct_spell_fr(comment:str) -> str:\n",
    "    \"\"\"\n",
    "    Correct the spelling of a french text\n",
    "    @Params:\n",
    "        comment         - required : str \n",
    "    \"\"\"\n",
    "\n",
    "    # Import library\n",
    "    from spellchecker import SpellChecker\n",
    "\n",
    "    # Instantiation\n",
    "    spell = SpellChecker(language='fr', distance=1)\n",
    "\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(comment.split())\n",
    "\n",
    "    for word in comment.split():\n",
    "        if word in misspelled_words:\n",
    "            correction = spell.correction(word)\n",
    "\n",
    "            if correction != None:\n",
    "                corrected_text.append(correction)\n",
    "            else:\n",
    "                corrected_text.append(word)\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "\n",
    "    return \" \".join(corrected_text)\n",
    "\n",
    "for i, comment in tqdm(x_train.iteritems()):\n",
    "    x_train[i] = correct_spell_fr(comment)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6880it [54:35,  2.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# 86 min d'exécution\n",
    "\n",
    "import spacy\n",
    "from spacy_lefff import LefffLemmatizer\n",
    "from spacy.language import Language\n",
    "\n",
    "# @Language.factory('french_lemmatizer')\n",
    "# def create_french_lemmatizer(nlp, name):\n",
    "#     return LefffLemmatizer()\n",
    "\n",
    "nlp = spacy.load('fr_dep_news_trf')\n",
    "nlp.add_pipe('french_lemmatizer', name='lefff')\n",
    "\n",
    "def fr_lemm(comment):\n",
    "    text_lemmatize = []\n",
    "    doc = nlp(comment)\n",
    "\n",
    "    for w in doc:\n",
    "        text_lemmatize.append(w.lemma_)\n",
    "\n",
    "    return \" \".join(text_lemmatize)\n",
    "\n",
    "for i, comment in tqdm(x_train.iteritems()):\n",
    "    x_train[i] = fr_lemm(comment)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# # Tokenize the x_train comments\n",
    "x_train = x_train.apply(word_tokenize)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the french stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = set(stopwords.words('french'))\n",
    "x_train = x_train.apply(lambda x: [word for word in x if word not in stop])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>\n",
    "<center><h2>Joblib Saving</h2></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A ce stade, nous sauvegardons l'ensemble de la progression pour éviter les deux calculs qui prennent du temps : spell checking et la lemmatization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_test_split_save_joblib/data_aug_test_exclude.joblib']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = { 'x_train': x_train,\n",
    "          'y_train': y_train,\n",
    "          'x_test': x_test,\n",
    "          'y_test': y_test\n",
    "}\n",
    "\n",
    "joblib.dump(model, 'train_test_split_save_joblib/data_aug_test_exclude.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test loading x_train\n",
    "\n",
    "# x_train = joblib.load('train_correct_save.joblib')['x_train']\n",
    "# y_train = joblib.load('train_correct_save.joblib')['y_train']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ee91b9fa71823173f867af0c8116fda06349ae6c05459be3443d30a809dfc65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
